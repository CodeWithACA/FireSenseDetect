import cv2
import os
import numpy as np
import onnxruntime as ort


class FusionInterface:
    """è§†é¢‘èžåˆæŽ¥å£ç±»ï¼Œç”¨äºŽå¤„ç†RGBå’ŒIRè§†é¢‘çš„å¯¹é½ã€èžåˆå’Œæ£€æµ‹ã€‚"""
    def __init__(self, mode='fuse', rgb_path=None, ir_path=None, output='results', 
                 ir_ahead=0, fusefunc='densefuse', confidence_thres=0.25, iou_thres=0.45, rknn=False):
        """
        åˆå§‹åŒ– FusionInterface ç±»çš„å®žä¾‹ã€‚
        å‚æ•°ï¼š
            mode: å¤„ç†æ¨¡å¼ï¼Œ'fuse'æˆ–'fuse_and_detect'
            rgb_path: RGBè§†é¢‘çš„è·¯å¾„
            ir_path: IRè§†é¢‘çš„è·¯å¾„
            output: è¾“å‡ºç»“æžœçš„ä¿å­˜è·¯å¾„
            ir_ahead: IRè§†é¢‘æ—©äºŽRGBè§†é¢‘çš„ç§’æ•°ï¼Œå¯ä¸ºè´Ÿæ•°
            fusefunc: èžåˆå‡½æ•°åç§°ï¼Œå¯é€‰å€¼ä¸º'densefuse'æˆ–'mfeif'
            confidence_thres: æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆä»…åœ¨fuse_and_detectæ¨¡å¼ä¸‹æœ‰æ•ˆï¼‰
            iou_thres: IOUé˜ˆå€¼ï¼ˆä»…åœ¨fuse_and_detectæ¨¡å¼ä¸‹æœ‰æ•ˆï¼‰
        """
        self.mode = mode
        self.rgb_path = rgb_path
        self.ir_path = ir_path
        self.output_path = output
        self.ir_ahead = ir_ahead if ir_ahead is not None else 0
        self.fusefunc_name = fusefunc
        self.confidence_thres = confidence_thres
        self.iou_thres = iou_thres
        self.rknn = rknn
        if self.rknn:
            from .DetectionInterfaceRKNN import RKNNDetection
            self.detection_interface = RKNNDetection(input_path = None, output_path = self.output_path, confidence_thres = self.confidence_thres, iou_thres = self.iou_thres)
        else:
            from .DetectionInterface import DetectionInterface
            self.detection_interface = DetectionInterface(input_path = None, output = self.output_path, confidence_thres = self.confidence_thres, iou_thres = self.iou_thres)
        # åˆ¤æ–­è¾“å‡ºè·¯å¾„æ˜¯æ–‡ä»¶è¿˜æ˜¯ç›®å½•
        self.output_is_file = False
        if self.output_path:
            _, ext = os.path.splitext(self.output_path)
            self.output_is_file = ext != ''
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if self.output_is_file:
            # å¦‚æžœæ˜¯æ–‡ä»¶ï¼Œç¡®ä¿å…¶çˆ¶ç›®å½•å­˜åœ¨
            output_dir = os.path.dirname(self.output_path)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
        else:
            # å¦‚æžœæ˜¯ç›®å½•ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(self.output_path, exist_ok=True)
        
        # æ ¹æ®fusefunc_nameé€‰æ‹©èžåˆå‡½æ•°
        if self.fusefunc_name == 'densefuse':
            self.fusion_func = self.densefuse_onnx
            # åŠ è½½DenseFuseæ¨¡åž‹
            self.encoder_session = ort.InferenceSession(
                r"./models/model_gray_encoder.onnx",
                providers=["CUDAExecutionProvider", "CPUExecutionProvider"] if ort.get_device() == "GPU" else ["CPUExecutionProvider"]
            )
            self.decoder_session = ort.InferenceSession(
                r"./models/model_gray_decoder.onnx",
                providers=["CUDAExecutionProvider", "CPUExecutionProvider"] if ort.get_device() == "GPU" else ["CPUExecutionProvider"]
            )
        elif self.fusefunc_name == 'mfeif':
            self.fusion_func = self.mfeif_onnx
            # åŠ è½½mfeifæ¨¡åž‹
            self.mfeif_session = ort.InferenceSession(
                r"./models/mfeif.onnx",
                providers=["CUDAExecutionProvider", "CPUExecutionProvider"] if ort.get_device() == "GPU" else ["CPUExecutionProvider"]
            )
        
        # å¦‚æžœæ˜¯fuse_and_detectæ¨¡å¼ï¼Œåˆå§‹åŒ–æ£€æµ‹æŽ¥å£
        if self.mode == 'fuse_and_detect':
            # åœ¨è¿™é‡Œå¯¼å…¥DetectionInterface
            from utils.DetectionInterface import DetectionInterface
            self.detection_interface = DetectionInterface(
                input_path=None,  # å°†åœ¨å¤„ç†æ¯ä¸€å¸§æ—¶åŠ¨æ€è®¾ç½®
                output=self.output_path,
                confidence_thres=self.confidence_thres,
                iou_thres=self.iou_thres
            )
    
    def densefuse_onnx(self, rgb_frame, ir_frame):
        """
        ä½¿ç”¨DenseFuseæ¨¡åž‹èžåˆRGBå’ŒIRå¸§
        å‚æ•°ï¼š
            rgb_frame: RGBå¸§
            ir_frame: IRå¸§
        è¿”å›žï¼š
            èžåˆåŽçš„å¸§
        """
        # èŽ·å–è¾“å…¥è¾“å‡ºåç§°
        encoder_input_name = self.encoder_session.get_inputs()[0].name
        encoder_output_name = self.encoder_session.get_outputs()[0].name
        decoder_input_name = self.decoder_session.get_inputs()[0].name
        decoder_output_name = self.decoder_session.get_outputs()[0].name
    
        # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
        if len(rgb_frame.shape) == 3:
            rgb_frame = cv2.cvtColor(rgb_frame, cv2.COLOR_BGR2GRAY)
        if len(ir_frame.shape) == 3:
            ir_frame = cv2.cvtColor(ir_frame, cv2.COLOR_BGR2GRAY)
    
        # ä½¿ç”¨OpenCVå’ŒNumPyæ›¿ä»£torchvisionçš„transforms
        # å½’ä¸€åŒ–å›¾åƒæ•°æ® (0-255 -> 0-1)
        ir_norm = ir_frame.astype(np.float32) / 255.0
        vis_norm = rgb_frame.astype(np.float32) / 255.0
        
        # æ·»åŠ é€šé“ç»´åº¦ (H,W) -> (1,H,W)
        ir_tensor = np.expand_dims(ir_norm, axis=0)
        vis_tensor = np.expand_dims(vis_norm, axis=0)
        
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦ (1,H,W) -> (1,1,H,W)
        ir_tensor = np.expand_dims(ir_tensor, axis=0)
        vis_tensor = np.expand_dims(vis_tensor, axis=0)
        
        # ä½¿ç”¨ç¼–ç å™¨
        ir_features = self.encoder_session.run([encoder_output_name], {encoder_input_name: ir_tensor})[0]
        vis_features = self.encoder_session.run([encoder_output_name], {encoder_input_name: vis_tensor})[0]
        # èžåˆç‰¹å¾
        fusion_features = (ir_features + vis_features) / 2
        # ä½¿ç”¨è§£ç å™¨
        fusion_image = self.decoder_session.run([decoder_output_name], {decoder_input_name: fusion_features})[0]
        # å¤„ç†èžåˆå›¾åƒ
        fusion_image = fusion_image[0]  # ç§»é™¤æ‰¹æ¬¡ç»´åº¦
        fusion_image = np.transpose(fusion_image, (1, 2, 0))  # CHW -> HWC
        fusion_image = (fusion_image * 255).clip(0, 255).astype(np.uint8)
        # å¦‚æžœæ˜¯å•é€šé“å›¾åƒï¼Œç§»é™¤é€šé“ç»´åº¦
        if fusion_image.shape[2] == 1:
            fusion_image = fusion_image[:, :, 0]
        # è½¬æ¢å›žä¸‰é€šé“å›¾åƒç”¨äºŽè§†é¢‘ä¿å­˜
        if len(fusion_image.shape) == 2:
            fusion_image = cv2.cvtColor(fusion_image, cv2.COLOR_GRAY2BGR)
        return fusion_image
    
    def mfeif_onnx(self, vi_input, ir_input):
        """
        ä½¿ç”¨mfeifæ¨¡åž‹èžåˆå¯è§å…‰å’Œçº¢å¤–å›¾åƒ
        å‚æ•°:
            vi_input: å¯è§å…‰å›¾åƒ(numpyæ•°ç»„)
            ir_input: çº¢å¤–å›¾åƒ(numpyæ•°ç»„)
        è¿”å›ž:
            èžåˆåŽçš„å›¾åƒ(numpyæ•°ç»„ï¼ŒBGRæ ¼å¼)
        """
        session = self.mfeif_session
        input_names = [input.name for input in session.get_inputs()]
        output_names = [output.name for output in session.get_outputs()]
        
        # å¤åˆ¶è¾“å…¥å›¾åƒ
        vi_image = vi_input.copy()
        ir_image = ir_input.copy()
        
        # è½¬æ¢ä¸ºç°åº¦å›¾
        if len(vi_image.shape) == 3:
            vi_image = cv2.cvtColor(vi_image, cv2.COLOR_BGR2GRAY)
        if len(ir_image.shape) == 3:
            ir_image = cv2.cvtColor(ir_image, cv2.COLOR_BGR2GRAY)
        
        # å›¾åƒé¢„å¤„ç†
        # å½’ä¸€åŒ–
        ir_tensor = ir_image.astype(np.float32) / 255.0
        vi_tensor = vi_image.astype(np.float32) / 255.0
        
        # è½¬æ¢ä¸ºCHWæ ¼å¼
        if len(ir_tensor.shape) == 2:
            ir_tensor = np.expand_dims(ir_tensor, axis=0)
        else:
            ir_tensor = np.transpose(ir_tensor, (2, 0, 1))
            
        if len(vi_tensor.shape) == 2:
            vi_tensor = np.expand_dims(vi_tensor, axis=0)
        else:
            vi_tensor = np.transpose(vi_tensor, (2, 0, 1))
        
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        ir_tensor = np.expand_dims(ir_tensor, axis=0)
        vi_tensor = np.expand_dims(vi_tensor, axis=0)
        
        # å‡†å¤‡è¾“å…¥
        ort_inputs = {
            input_names[0]: ir_tensor,
            input_names[1]: vi_tensor
        }
        
        # è¿è¡ŒæŽ¨ç†
        ort_outputs = session.run(output_names, ort_inputs)
        fused = ort_outputs[0]
        
        # å¤„ç†èžåˆå›¾åƒ
        fused = fused[0]  # ç§»é™¤æ‰¹æ¬¡ç»´åº¦
        
        # å¦‚æžœæ˜¯å•é€šé“å›¾åƒ
        if fused.shape[0] == 1:
            # ç§»é™¤é€šé“ç»´åº¦
            fused_image = fused[0]
        else:
            # è½¬æ¢ä¸ºHWCæ ¼å¼
            fused_image = np.transpose(fused, (1, 2, 0))
        
        # åå½’ä¸€åŒ–å¹¶è½¬æ¢ä¸ºuint8
        fused_image = (fused_image * 255.0).clip(0, 255).astype(np.uint8)
        
        # è½¬æ¢å›žä¸‰é€šé“å›¾åƒ
        if len(fused_image.shape) == 2:
            fused_image = cv2.cvtColor(fused_image, cv2.COLOR_GRAY2BGR)
        
        return fused_image
    
    def _is_compatible_extension(self, input_ext, output_ext):
        """
        æ£€æŸ¥è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶æ‰©å±•åæ˜¯å¦å…¼å®¹
        """
        input_ext = input_ext.lower()
        output_ext = output_ext.lower()
        
        # è§†é¢‘æ‰©å±•ååˆ—è¡¨
        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv']
        # å›¾åƒæ‰©å±•ååˆ—è¡¨
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']
        
        # å¦‚æžœè¾“å…¥æ˜¯è§†é¢‘ï¼Œè¾“å‡ºä¹Ÿåº”è¯¥æ˜¯è§†é¢‘
        if input_ext in video_extensions:
            return output_ext in video_extensions
        # å¦‚æžœè¾“å…¥æ˜¯å›¾åƒï¼Œè¾“å‡ºä¹Ÿåº”è¯¥æ˜¯å›¾åƒ
        elif input_ext in image_extensions:
            return output_ext in image_extensions
        
        # é»˜è®¤æƒ…å†µä¸‹ï¼Œè®¤ä¸ºä¸å…¼å®¹
        return False
    
    def fuse(self):
        """
        æ‰§è¡Œè§†é¢‘å¯¹é½å’Œèžåˆï¼Œå¦‚æžœæ˜¯fuse_and_detectæ¨¡å¼è¿˜ä¼šè¿›è¡Œæ£€æµ‹
        è¿”å›žï¼š
            output_path: å¤„ç†åŽçš„è§†é¢‘ä¿å­˜è·¯å¾„
        """
        # æ‰“å¼€è§†é¢‘æ–‡ä»¶
        ir_cap = cv2.VideoCapture(self.ir_path)
        rgb_cap = cv2.VideoCapture(self.rgb_path)
        
        if not ir_cap.isOpened() or not rgb_cap.isOpened():
            print(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {self.ir_path} æˆ– {self.rgb_path}")
            return None
        
        # èŽ·å–è§†é¢‘ä¿¡æ¯
        rgb_fps = 25
        ir_fps = 30
        
        # è®¡ç®—æ€»å¸§æ•°
        rgb_frame_count = int(rgb_cap.get(cv2.CAP_PROP_FRAME_COUNT))
        ir_frame_count = int(ir_cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # è®¾ç½®è¾“å‡ºè§†é¢‘çš„å®½é«˜ä¸ºå¤„ç†åŽçš„å°ºå¯¸
        output_width = 640  # è£å‰ªåŽçš„å®½åº¦ (672-32)
        output_height = 418  # è£å‰ªåŽçš„é«˜åº¦ (485-66)
        
        # åˆ›å»ºè¾“å‡ºè§†é¢‘æ–‡ä»¶å
        base_name = os.path.basename(self.rgb_path)
        name, ext = os.path.splitext(base_name)
        
        # æ ¹æ®è¾“å‡ºè·¯å¾„æ˜¯æ–‡ä»¶è¿˜æ˜¯ç›®å½•æ¥ç¡®å®šè¾“å‡ºè§†é¢‘è·¯å¾„
        if self.output_is_file:
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ‰©å±•åæ˜¯å¦ä¸Žè¾“å…¥æ–‡ä»¶ç±»åž‹åŒ¹é…
            _, output_ext = os.path.splitext(self.output_path)
            if not self._is_compatible_extension(ext, output_ext):
                print(f"é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶ä¸ºè§†é¢‘({ext})ï¼Œä½†è¾“å‡ºæ–‡ä»¶æ‰©å±•å({output_ext})ä¸å…¼å®¹")
                return None
            output_video_path = self.output_path
        else:
            output_video_path = os.path.join(self.output_path, f"fused_{name}{ext}")
        
        # åˆ›å»ºè¾“å‡ºè§†é¢‘
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_video_path, fourcc, rgb_fps, (output_width, output_height))
        
        # é¢„è¯»å–æ‰€æœ‰IRå¸§åŠå…¶æ—¶é—´æˆ³
        ir_frames = []
        ir_timestamps = []
        
        print("æ­£åœ¨è¯»å–IRè§†é¢‘å¸§...")
        frame_idx = 0
        while True:
            ret, frame = ir_cap.read()
            if not ret:
                break
            
            # æ˜¾ç¤ºIRå¸§è¯»å–è¿›åº¦
            print(f"\rè¯»å–IRå¸§è¿›åº¦: {frame_idx + 1}/{ir_frame_count} ({(frame_idx + 1)/ir_frame_count*100:.1f}%)", end="")
            
            # è®¡ç®—å®žé™…æ—¶é—´æˆ³
            actual_timestamp = frame_idx * (1.0 / ir_fps)
            ir_frames.append(frame)
            ir_timestamps.append(actual_timestamp)
            frame_idx += 1
        
        print(f"\nIRè§†é¢‘å…±æœ‰ {len(ir_frames)} å¸§")
        
        # å¤„ç†RGBè§†é¢‘çš„æ¯ä¸€å¸§
        print("\næ­£åœ¨å¤„ç†RGBè§†é¢‘å¸§...")
        rgb_frame_idx = 0
        successful_fusions = 0  # è®°å½•æˆåŠŸèžåˆçš„å¸§æ•°
        time_threshold = 0.05  # æ—¶é—´å·®é˜ˆå€¼ï¼Œå•ä½ä¸ºç§’

        import time
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        print("å¼€å§‹æ—¶é—´:", start_time)

        while True:
            ret, rgb_frame = rgb_cap.read()
            if not ret:
                break
                
            # æ˜¾ç¤ºå¤„ç†è¿›åº¦
            print(f"\rå¤„ç†è¿›åº¦: {rgb_frame_idx + 1}/{rgb_frame_count} ({(rgb_frame_idx + 1)/rgb_frame_count*100:.1f}%), å·²æˆåŠŸèžåˆ: {successful_fusions}å¸§", end="")
            
            # è®¡ç®—RGBå¸§çš„æ—¶é—´æˆ³ï¼ˆè€ƒè™‘æ—¶é—´å·®ï¼‰
            rgb_timestamp = rgb_frame_idx * (1.0 / rgb_fps) - self.ir_ahead
            
            # ç›´æŽ¥è®¡ç®—é¢„ä¼°çš„IRå¸§ç´¢å¼•
            estimated_ir_idx = int(rgb_timestamp * ir_fps)
            
            # å¦‚æžœé¢„ä¼°ç´¢å¼•å·²ç»è¶…å‡ºIRå¸§èŒƒå›´ï¼Œåˆ™è·³è¿‡å½“å‰RGBå¸§
            if estimated_ir_idx >= len(ir_timestamps) or estimated_ir_idx < 0:
                print(f"\nè·³è¿‡RGBå¸§ {rgb_frame_idx+1}/{rgb_frame_count}, IRå¸§ç´¢å¼•è¶…å‡ºèŒƒå›´")
                rgb_frame_idx += 1
                continue
            
            # åœ¨é¢„ä¼°ä½ç½®é™„è¿‘æŸ¥æ‰¾æœ€ä½³åŒ¹é…ï¼ˆæœç´¢èŒƒå›´è®¾ä¸ºå‰åŽ5å¸§ï¼‰
            search_start = max(0, estimated_ir_idx - 5)
            search_end = min(len(ir_timestamps), estimated_ir_idx + 5)
            
            # ç¡®ä¿æœç´¢èŒƒå›´æœ‰æ•ˆ
            if search_start >= len(ir_timestamps) or search_end <= search_start:
                print(f"\nè·³è¿‡RGBå¸§ {rgb_frame_idx+1}/{rgb_frame_count}, æ— æ•ˆçš„æœç´¢èŒƒå›´")
                rgb_frame_idx += 1
                continue
                
            # åœ¨å±€éƒ¨èŒƒå›´å†…æ‰¾æœ€ä½³åŒ¹é…
            closest_ir_idx = search_start
            min_time_diff = abs(ir_timestamps[search_start] - rgb_timestamp)
            
            for i in range(search_start + 1, search_end):
                time_diff = abs(ir_timestamps[i] - rgb_timestamp)
                if time_diff < min_time_diff:
                    min_time_diff = time_diff
                    closest_ir_idx = i
            
            # å¦‚æžœæ‰¾åˆ°æ—¶é—´å·®åœ¨é˜ˆå€¼å†…çš„IRå¸§ï¼Œåˆ™è¿›è¡Œèžåˆ
            if min_time_diff <= time_threshold:
                # èŽ·å–å¯¹åº”çš„IRå¸§
                ir_frame = ir_frames[closest_ir_idx]
                
                # å¯¹RGBå¸§è¿›è¡Œè°ƒæ•´å¤§å°å’Œè£å‰ª
                rgb_resized = cv2.resize(rgb_frame, (704, 419), cv2.INTER_AREA)
                rgb_cropped = rgb_resized[:, 32:672]
                
                # å¯¹IRå¸§è¿›è¡Œè£å‰ª
                ir_cropped = ir_frame[66:485, :]
                
                # ç¡®ä¿IRå¸§ä¸ŽRGBå¸§å¤§å°ä¸€è‡´
                if ir_cropped.shape[1] != rgb_cropped.shape[1] or ir_cropped.shape[0] != rgb_cropped.shape[0]:
                    ir_cropped = cv2.resize(ir_cropped, (rgb_cropped.shape[1], rgb_cropped.shape[0]))
                
                # å°†å¤„ç†åŽçš„å¸§é€å…¥èžåˆå‡½æ•°
                fused_frame = self.fusion_func(rgb_cropped, ir_cropped)
                
                # ç¡®ä¿èžåˆåŽçš„å¸§å°ºå¯¸æ­£å¥½æ˜¯640x418
                if fused_frame.shape[1] != 640 or fused_frame.shape[0] != 418:
                    fused_frame = cv2.resize(fused_frame, (640, 418), cv2.INTER_AREA)
                
                # å¦‚æžœæ˜¯fuse_and_detectæ¨¡å¼ï¼Œå¯¹èžåˆåŽçš„å¸§è¿›è¡Œæ£€æµ‹
                if self.mode == 'fuse_and_detect':
                    # ç›´æŽ¥ä½¿ç”¨detect_frameæ–¹æ³•å¤„ç†èžåˆåŽçš„å¸§ï¼Œé¿å…ä¿å­˜å’Œé‡æ–°åŠ è½½
                    if self.rknn:
                        detected_frame = self.detection_interface.detect_frame(fused_frame)
                    else:
                        detected_frame = self.detection_interface.detect_frame(fused_frame)
                    
                    # å†™å…¥æ£€æµ‹åŽçš„å¸§
                    out.write(detected_frame)
                else:
                    # ç›´æŽ¥å†™å…¥èžåˆåŽçš„å¸§
                    out.write(fused_frame)
                
                successful_fusions += 1
            else:
                print(f"\nè·³è¿‡RGBå¸§ {rgb_frame_idx+1}/{rgb_frame_count}, æœªæ‰¾åˆ°åŒ¹é…çš„IRå¸§ (æœ€å°æ—¶é—´å·®: {min_time_diff:.4f}ç§’)")
            
            rgb_frame_idx += 1
        
        # è®¡ç®—å¹¶æ‰“å°æ€»è€—æ—¶
        print(f"\næ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")

        # é‡Šæ”¾èµ„æº
        ir_cap.release()
        rgb_cap.release()
        out.release()
        
        print(f"\nå¤„ç†å®Œæˆï¼Œå…±å¤„ç† {rgb_frame_count} å¸§ï¼ŒæˆåŠŸèžåˆ {successful_fusions} å¸§ ({successful_fusions/rgb_frame_count*100:.1f}%)ï¼Œè¾“å‡ºè§†é¢‘ä¿å­˜è‡³ {output_video_path}")
        return output_video_path
    
    def main(self):
        """
        ä¸»å‡½æ•°ï¼Œæ‰§è¡Œèžåˆå’Œæ£€æµ‹
        è¿”å›žï¼š
            output_path: å¤„ç†åŽçš„è§†é¢‘ä¿å­˜è·¯å¾„
        """
        print(f"è§†é¢‘èžåˆ ðŸš€ æ¨¡å¼: {self.mode}")
        print(f"RGBè§†é¢‘: {self.rgb_path}")
        print(f"IRè§†é¢‘: {self.ir_path}")
        print(f"è¾“å‡ºè·¯å¾„: {self.output_path}")
        print(f"IRæå‰æ—¶é—´: {self.ir_ahead}ç§’")
        print(f"èžåˆå‡½æ•°: {self.fusefunc_name}")
        
        if self.mode == 'fuse_and_detect':
            print(f"æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼: {self.confidence_thres}")
            print(f"IOUé˜ˆå€¼: {self.iou_thres}")
        
        # æ‰§è¡Œèžåˆå¹¶è¿”å›žä¿å­˜è·¯å¾„
        return self.fuse()

if __name__ == "__main__":
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„ï¼Œä»¥ä¾¿èƒ½å¤Ÿæ­£ç¡®å¯¼å…¥æ¨¡å—
    import sys
    import os
    # èŽ·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰
    root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    # å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„
    sys.path.append(root_dir)
    
    # åœ¨è¿™é‡Œå¯¼å…¥DetectionInterfaceç”¨äºŽæµ‹è¯•
    from utils.DetectionInterface import DetectionInterface
    
    # æµ‹è¯•ä»£ç 
    fusion = FusionInterface(
        mode='fuse_and_detect',
        rgb_path=r"demo\output_rgb_smoked3.mp4",
        ir_path=r"demo\output_tr_smoked3.mp4",
        output=r"results",
        ir_ahead=-4.55,
        fusefunc='densefuse'
    )
    output_path = fusion.main()
    print(f"å¤„ç†å®Œæˆï¼Œç»“æžœå·²ä¿å­˜è‡³: {output_path}")